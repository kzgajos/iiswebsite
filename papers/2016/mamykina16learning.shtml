<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<!--#include virtual="../../common-head.html" --> 
<!--#include virtual="../../common-js.html" --> 
<style type="text/css">
a#n-pubs { 
<!--#include virtual="../../common-active-style.html" --> 
}
</style>

<title>Learning From the Crowd: Observational Learning in Crowdsourcing Communities</title>
<meta name="twitter:card" content="summary" />

<meta property="og:title" content="Learning From the Crowd: Observational Learning in Crowdsourcing Communities">
<meta property="og:description" content="Crowd work provides solutions to complex problems effectively, efficiently, and at low cost. Previous research showed that feedback, particularly correctness feedback can help crowd workers improve their performance; yet such feedback, particularly when generated by experts, is costly and difficult to scale. In our research we investigate approaches to facilitating continuous observational learning in crowdsourcing communities. In a study conducted with workers on Amazon Mechanical Turk, we asked workers to complete a set of tasks identifying nutritional composition of different meals. We examined workers' accuracy gains after being exposed to expert-generated feedback and to two types of peer-generated feedback: direct accuracy assessment with explanations of errors, and a comparison with solutions generated by other workers. The study further confirmed that expert-generated feedback is a powerful mechanism for facilitating learning and leads to significant gains in accuracy. However, the study also showed that comparing one's own solutions with a variety of solutions suggested by others and their comparative frequencies leads to significant gains in accuracy. This solution is particularly attractive because of its low cost, minimal impact on time and cost of job completion, and high potential for adoption by a variety of crowdsourcing platforms.">

 	<script src="media/share.js"></script>
    <script>
        $(function() {
            share.makeButtons("#share");
        });
    </script>
</head>


<body>

<!--#include virtual="../../common-top.html" --> 

<h1 class="head">Learning From the Crowd: Observational Learning in Crowdsourcing Communities</h1>
<h2 class="head">Lena Mamykina, Thomas&nbsp;N. Smyth, Jill&nbsp;P. Dimond, and Krzysztof&nbsp;Z. Gajos</h2>

<!--#include virtual="../../common-nav.html" --> 



<br clear="all"/>

<div style="text-align: right" id="share"></div>


<h3>Abstract</h3>
<div class="content">

Crowd work provides solutions to complex problems effectively, efficiently, and at low cost. Previous research showed that feedback, particularly correctness feedback can help crowd workers improve their performance; yet such feedback, particularly when generated by experts, is costly and difficult to scale. In our research we investigate approaches to facilitating continuous observational learning in crowdsourcing communities. In a study conducted with workers on Amazon Mechanical Turk, we asked workers to complete a set of tasks identifying nutritional composition of different meals. We examined workers' accuracy gains after being exposed to expert-generated feedback and to two types of peer-generated feedback: direct accuracy assessment with explanations of errors, and a comparison with solutions generated by other workers. The study further confirmed that expert-generated feedback is a powerful mechanism for facilitating learning and leads to significant gains in accuracy. However, the study also showed that comparing one's own solutions with a variety of solutions suggested by others and their comparative frequencies leads to significant gains in accuracy. This solution is particularly attractive because of its low cost, minimal impact on time and cost of job completion, and high potential for adoption by a variety of crowdsourcing platforms.
<br clear="right"/>
</div>




<h3>Available Versions</h3>
<div class="content">
<ul>
<li><a href="papers/2016/mamykina16learning.pdf">Authors' version (pdf)</a></li>

<li><a class="external" href="http://dx.doi.org/10.1145/2858036.2858560">Publisher's version
 (ACM)
</a></li>


<!--??url??<li><a href="##url##">Other</a></li>??/url??-->
</ul>
</div>







<h3>Citation Information</h3>
<div class="content">
<p>Lena Mamykina, Thomas&nbsp;N. Smyth, Jill&nbsp;P. Dimond, and Krzysztof&nbsp;Z. Gajos. Learning from the crowd: Observational learning in crowdsourcing communities. In <EM>Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</EM>, CHI '16, pages 2635-2644, New York, NY, USA, 2016. ACM.</p>

<A href='#' class='actionbutton' onclick="var w=window.open('','BibTeX: mamykina16:learning','scrollbars=yes,menubar=no,height=200,width=600,resizable=yes,toolbar=no,status=no');w.document.writeln('<HTML><HEAD><TITLE>BibTeX: mamykina16:learning</TITLE></HEAD><BODY><PRE>@inproceedings{mamykina16:learning,\n  author = {Mamykina, Lena and Smyth, Thomas N. and Dimond, Jill P. and Gajos, Krzysztof Z.},\n  title = {Learning From the Crowd: Observational Learning in Crowdsourcing Communities},\n  booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},\n  series = {CHI \'16},\n  year = {2016},\n  isbn = {978-1-4503-3362-7},\n  location = {Santa Clara, California, USA},\n  pages = {2635--2644},\n  numpages = {10},\n    doi = {10.1145/2858036.2858560},\n  acmid = {2858560},\n  publisher = {ACM},\n  address = {New York, NY, USA},\n  keywords = {crowdsourcing, nutritional assessment, observational learning},\n  abstract = {Crowd work provides solutions to complex problems effectively, efficiently, and at low cost. Previous research showed that feedback, particularly correctness feedback can help crowd workers improve their performance; yet such feedback, particularly when generated by experts, is costly and difficult to scale. In our research we investigate approaches to facilitating continuous observational learning in crowdsourcing communities. In a study conducted with workers on Amazon Mechanical Turk, we asked workers to complete a set of tasks identifying nutritional composition of different meals. We examined workers\' accuracy gains after being exposed to expert-generated feedback and to two types of peer-generated feedback: direct accuracy assessment with explanations of errors, and a comparison with solutions generated by other workers. The study further confirmed that expert-generated feedback is a powerful mechanism for facilitating learning and leads to significant gains in accuracy. However, the study also showed that comparing one\'s own solutions with a variety of solutions suggested by others and their comparative frequencies leads to significant gains in accuracy. This solution is particularly attractive because of its low cost, minimal impact on time and cost of job completion, and high potential for adoption by a variety of crowdsourcing platforms.},\n      }\n\n</PRE></BODY></HTML>');w.document.close();return false">BibTeX</A>
</div>

<br clear="all"/>
<hr/>
<div id="share"></div>
</div> <!-- main -->


<!--#include virtual="../../common-footer.html" --> 

</body>
</html>
