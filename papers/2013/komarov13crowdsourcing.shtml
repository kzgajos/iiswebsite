<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<!--#include virtual="../../common-head.html" --> 
<!--#include virtual="../../common-js.html" --> 
<style type="text/css">
a#n-pubs { 
<!--#include virtual="../../common-active-style.html" --> 
}
</style>

<title>Crowdsourcing performance evaluations of user interfaces</title>
<meta name="twitter:card" content="summary" />
<meta property="og:image" content="http://iis.seas.harvard.edu/projects/images/LABvsMT.png">
<meta property="og:title" content="Crowdsourcing performance evaluations of user interfaces">
<meta property="og:description" content="Online labor markets, such as Amazon's Mechanical Turk (MTurk), provide an attractive platform for conducting human subjects experiments because the relative ease of recruitment, low cost, and a diverse pool of potential participants enable larger-scale experimentation and faster experimental revision cycle compared to lab-based settings. However, because the experimenter gives up the direct control over the participants' environments and behavior, concerns about the quality of the data collected in online settings are pervasive. In this paper, we investigate the feasibility of conducting online performance evaluations of user interfaces with anonymous, unsupervised, paid participants recruited via MTurk. We implemented three performance experiments to re-evaluate three previously well-studied user interface designs. We conducted each experiment both in lab and online with participants recruited via MTurk. The analysis of our results did not yield any evidence of significant or substantial differences in the data collected in the two settings: All statistically significant differences detected in lab were also present on MTurk and the effect sizes were similar. In addition, there were no significant differences between the two settings in the raw task completion times, error rates, consistency, or the rates of utilization of the novel interaction mechanisms introduced in the experiments. These results suggest that MTurk may be a productive setting for conducting performance evaluations of user interfaces providing a complementary approach to existing methodologies.">

 	<script src="media/share.js"></script>
    <script>
        $(function() {
            share.makeButtons("#share");
        });
    </script>
</head>


<body>

<!--#include virtual="../../common-top.html" --> 

<h1 class="head">Crowdsourcing performance evaluations of user interfaces</h1>
<h2 class="head">Steven Komarov, Katharina Reinecke, and Krzysztof&nbsp;Z. Gajos</h2>

<!--#include virtual="../../common-nav.html" --> 



<br clear="all"/>

<div style="text-align: right" id="share"></div>


<h3>Abstract</h3>
<div class="content">
<img style="margin-left: 15px; margin-bottom: 10px; max-width: 300px" align="right" alt="" src="http://iis.seas.harvard.edu/projects/images/LABvsMT.png"/>
Online labor markets, such as Amazon's Mechanical Turk (MTurk), provide an attractive platform for conducting human subjects experiments because the relative ease of recruitment, low cost, and a diverse pool of potential participants enable larger-scale experimentation and faster experimental revision cycle compared to lab-based settings. However, because the experimenter gives up the direct control over the participants' environments and behavior, concerns about the quality of the data collected in online settings are pervasive. In this paper, we investigate the feasibility of conducting online performance evaluations of user interfaces with anonymous, unsupervised, paid participants recruited via MTurk. We implemented three performance experiments to re-evaluate three previously well-studied user interface designs. We conducted each experiment both in lab and online with participants recruited via MTurk. The analysis of our results did not yield any evidence of significant or substantial differences in the data collected in the two settings: All statistically significant differences detected in lab were also present on MTurk and the effect sizes were similar. In addition, there were no significant differences between the two settings in the raw task completion times, error rates, consistency, or the rates of utilization of the novel interaction mechanisms introduced in the experiments. These results suggest that MTurk may be a productive setting for conducting performance evaluations of user interfaces providing a complementary approach to existing methodologies.
<br clear="right"/>
</div>




<h3>Available Versions</h3>
<div class="content">
<ul>
<li><a href="papers/2013/komarov13crowdsourcing.pdf">Authors' version (pdf)</a></li>

<li><a class="external" href="http://dl.acm.org/citation.cfm?doid=2470654.2470684">Publisher's version
 (ACM)
</a></li>


<!--??url??<li><a href="##url##">Other</a></li>??/url??-->
</ul>
</div>




<h3>Additional Resources</h3>
<div class="content">
<a href="http://iis.seas.harvard.edu/resources/">Data</a>
</div>




<h3>Citation Information</h3>
<div class="content">
<p>Steven Komarov, Katharina Reinecke, and Krzysztof&nbsp;Z. Gajos. Crowdsourcing performance evaluations of user interfaces. In <EM>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</EM>, CHI '13, pages 207-216, New York, NY, USA, 2013. ACM.</p>

<A href='#' class='actionbutton' onclick="var w=window.open('','BibTeX: komarov13:crowdsourcing','scrollbars=yes,menubar=no,height=200,width=600,resizable=yes,toolbar=no,status=no');w.document.writeln('<HTML><HEAD><TITLE>BibTeX: komarov13:crowdsourcing</TITLE></HEAD><BODY><PRE>@inproceedings{komarov13:crowdsourcing,\n  author = {Komarov, Steven and Reinecke, Katharina and Gajos, Krzysztof Z.},\n  title = {Crowdsourcing performance evaluations of user interfaces},\n  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n  series = {CHI \'13},\n  year = {2013},\n  isbn = {978-1-4503-1899-0},\n  location = {Paris, France},\n  pages = {207--216},\n  numpages = {10},\n    doi = {10.1145/2470654.2470684},\n  acmid = {2470684},\n  publisher = {ACM},\n  address = {New York, NY, USA},\n  abstract = {Online labor markets, such as Amazon\'s Mechanical Turk (MTurk), provide an attractive platform for conducting human subjects experiments because the relative ease of recruitment, low cost, and a diverse pool of potential participants enable larger-scale experimentation and faster experimental revision cycle compared to lab-based settings. However, because the experimenter gives up the direct control over the participants\' environments and behavior, concerns about the quality of the data collected in online settings are pervasive. In this paper, we investigate the feasibility of conducting online performance evaluations of user interfaces with anonymous, unsupervised, paid participants recruited via MTurk. We implemented three performance experiments to re-evaluate three previously well-studied user interface designs. We conducted each experiment both in lab and online with participants recruited via MTurk. The analysis of our results did not yield any evidence of significant or substantial differences in the data collected in the two settings: All statistically significant differences detected in lab were also present on MTurk and the effect sizes were similar. In addition, there were no significant differences between the two settings in the raw task completion times, error rates, consistency, or the rates of utilization of the novel interaction mechanisms introduced in the experiments. These results suggest that MTurk may be a productive setting for conducting performance evaluations of user interfaces providing a complementary approach to existing methodologies.},\n            }\n\n</PRE></BODY></HTML>');w.document.close();return false">BibTeX</A>
</div>

<br clear="all"/>
<hr/>
<div id="share"></div>
</div> <!-- main -->


<!--#include virtual="../../common-footer.html" --> 

</body>
</html>
