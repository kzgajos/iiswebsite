<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<!--#include virtual="../../common-head.html" --> 
<!--#include virtual="../../common-js.html" --> 
<style type="text/css">
a#n-pubs { 
<!--#include virtual="../../common-active-style.html" --> 
}
</style>

<title>To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-Assisted Decision-Making</title>
<meta name="twitter:card" content="summary" />

<meta property="og:title" content="To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-Assisted Decision-Making">
<meta property="og:description" content="People supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI's suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.">

 	<script src="media/share.js"></script>
    <script>
        $(function() {
            share.makeButtons("#share");
        });
    </script>
</head>


<body>

<!--#include virtual="../../common-top.html" --> 

<h1 class="head">To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-Assisted Decision-Making</h1>
<h2 class="head">Zana Bu&#231;inca, Maja&nbsp;Barbara Malaya, and Krzysztof&nbsp;Z. Gajos</h2>

<!--#include virtual="../../common-nav.html" --> 



<br clear="all"/>

<div style="text-align: right" id="share"></div>


<h3>Abstract</h3>
<div class="content">

People supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI's suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.
<br clear="right"/>
</div>




<h3>Available Versions</h3>
<div class="content">
<ul>
<li><a href="papers/2021/bucinca21trust.pdf">Authors' version (pdf)</a></li>




<!--??url??<li><a href="##url##">Other</a></li>??/url??-->
</ul>
</div>







<h3>Citation Information</h3>
<div class="content">
<p>Zana Bu&#231;inca, Maja&nbsp;Barbara Malaya, and Krzysztof&nbsp;Z. Gajos. To trust or to think: Cognitive forcing functions can reduce overreliance on ai in ai-assisted decision-making. <EM>Proc. ACM Hum.-Comput. Interact.</EM>, 5(CSCW1), April 2021.</p>

<A href='#' class='actionbutton' onclick="var w=window.open('','BibTeX: bucinca2021trust','scrollbars=yes,menubar=no,height=200,width=600,resizable=yes,toolbar=no,status=no');w.document.writeln('<HTML><HEAD><TITLE>BibTeX: bucinca2021trust</TITLE></HEAD><BODY><PRE>@article{bucinca2021trust,\n  author = {Bu\c{c}inca, Zana and Malaya, Maja Barbara and Gajos, Krzysztof Z.},\n  title = {To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-Assisted Decision-Making},\n  year = {2021},\n  issue_date = {April 2021},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  volume = {5},\n  number = {CSCW1},\n    doi = {10.1145/3449287},\n  abstract = {People supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI\'s suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.},\n  journal = {Proc. ACM Hum.-Comput. Interact.},\n  month = {April},\n  articleno = {188},\n  numpages = {21},\n  keywords = {trust, explanations, cognition, artificial intelligence},\n    }\n\n</PRE></BODY></HTML>');w.document.close();return false">BibTeX</A>
</div>

<br clear="all"/>
<hr/>
<div id="share"></div>
</div> <!-- main -->


<!--#include virtual="../../common-footer.html" --> 

</body>
</html>
