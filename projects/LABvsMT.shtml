<a name="first"></a>
<h4 class="ptitle">Crowdsourcing Performance Evaluations of User Interfaces</h4>

<p>
<img class="thumbleft" src="http://iis.seas.harvard.edu/projects/images/LABvsMT.png" alt=""
width="200" align="left"/>
Can computer users be trusted to paricipate in user interface studies from the comfort of their home? 
Can user interface researchers give up control over their subjects' environment? In this project we study whether we can use Amazon Mechanical Turk to conduct user interface studies reliably. 
To do so, we replicated three previously known performance experiments, the "Bubble Cursor," the "Split Menus," and the "Split Interface," both in our lab and on Mechanical Turk. 
We compared the lab with the online population in terms of performance metrics such as speed, accuracy, and consistency. 
The results show that the Mechanical Turk participants perform just as well as the lab participants. 
</p>
<div class="ppapers">
<P class="paper" id='xkomarov13:crowdsourcingx'>Steven Komarov, Katharina Reinecke, and Krzysztof&nbsp;Z. Gajos. <a href="papers/2013/komarov13crowdsourcing.pdf">Crowdsourcing performance evaluations of user interfaces</a>. In <EM>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</EM>, CHI '13, pages 207-216, New York, NY, USA, 2013. ACM.

<br/><span class="paperdetails">[<a href="papers/2013/komarov13crowdsourcing.shtml">Abstract, BibTeX, Data,  etc.</a>]</span></P>
</div>
<br clear="all" />



 
